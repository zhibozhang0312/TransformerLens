<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="transformer_lens.past_key_value_caching" href="transformer_lens.past_key_value_caching.html" /><link rel="prev" title="transformer_lens.hook_points" href="transformer_lens.hook_points.html" />

    <link rel="shortcut icon" href="../../_static/favicon.ico"/><!-- Generated with Sphinx 5.2.3 and Furo 2023.03.27 -->
        <title>transformer_lens.loading_from_pretrained - TransformerLens Documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">TransformerLens Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/transformer_lens_logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">TransformerLens Documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../content/getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/getting_started_mech_interp.html">Getting Started in Mechanistic Interpretability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/gallery.html">Gallery</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="modules.html">Transformer Lens API</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current has-children"><a class="reference internal" href="transformer_lens.html">transformer_lens</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.ActivationCache.html">transformer_lens.ActivationCache</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.BertNextSentencePrediction.html">transformer_lens.BertNextSentencePrediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.FactoredMatrix.html">transformer_lens.FactoredMatrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.HookedEncoder.html">transformer_lens.HookedEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.HookedEncoderDecoder.html">transformer_lens.HookedEncoderDecoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.HookedTransformer.html">transformer_lens.HookedTransformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.HookedTransformerConfig.html">transformer_lens.HookedTransformerConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.SVDInterpreter.html">transformer_lens.SVDInterpreter</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.evals.html">transformer_lens.evals</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.head_detector.html">transformer_lens.head_detector</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.hook_points.html">transformer_lens.hook_points</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">transformer_lens.loading_from_pretrained</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.past_key_value_caching.html">transformer_lens.past_key_value_caching</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.patching.html">transformer_lens.patching</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.train.html">transformer_lens.train</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.utils.html">transformer_lens.utils</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="transformer_lens.components.html">transformer_lens.components</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.abstract_attention.html">transformer_lens.components.abstract_attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.attention.html">transformer_lens.components.attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.bert_block.html">transformer_lens.components.bert_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.bert_embed.html">transformer_lens.components.bert_embed</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.bert_mlm_head.html">transformer_lens.components.bert_mlm_head</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.bert_nsp_head.html">transformer_lens.components.bert_nsp_head</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.bert_pooler.html">transformer_lens.components.bert_pooler</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.embed.html">transformer_lens.components.embed</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.grouped_query_attention.html">transformer_lens.components.grouped_query_attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.layer_norm.html">transformer_lens.components.layer_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.layer_norm_pre.html">transformer_lens.components.layer_norm_pre</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.pos_embed.html">transformer_lens.components.pos_embed</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.rms_norm.html">transformer_lens.components.rms_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.rms_norm_pre.html">transformer_lens.components.rms_norm_pre</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.t5_attention.html">transformer_lens.components.t5_attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.t5_block.html">transformer_lens.components.t5_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.token_typed_embed.html">transformer_lens.components.token_typed_embed</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.transformer_block.html">transformer_lens.components.transformer_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.components.unembed.html">transformer_lens.components.unembed</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="transformer_lens.pretrained.html">transformer_lens.pretrained</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4 has-children"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.html">transformer_lens.pretrained.weight_conversions</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.bert.html">transformer_lens.pretrained.weight_conversions.bert</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.bloom.html">transformer_lens.pretrained.weight_conversions.bloom</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.coder.html">transformer_lens.pretrained.weight_conversions.coder</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.gemma.html">transformer_lens.pretrained.weight_conversions.gemma</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.gpt2.html">transformer_lens.pretrained.weight_conversions.gpt2</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.gptj.html">transformer_lens.pretrained.weight_conversions.gptj</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.llama.html">transformer_lens.pretrained.weight_conversions.llama</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.mingpt.html">transformer_lens.pretrained.weight_conversions.mingpt</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.mistral.html">transformer_lens.pretrained.weight_conversions.mistral</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.mixtral.html">transformer_lens.pretrained.weight_conversions.mixtral</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.nanogpt.html">transformer_lens.pretrained.weight_conversions.nanogpt</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.neel_solu_old.html">transformer_lens.pretrained.weight_conversions.neel_solu_old</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.neo.html">transformer_lens.pretrained.weight_conversions.neo</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.neox.html">transformer_lens.pretrained.weight_conversions.neox</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.opt.html">transformer_lens.pretrained.weight_conversions.opt</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.phi.html">transformer_lens.pretrained.weight_conversions.phi</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.phi3.html">transformer_lens.pretrained.weight_conversions.phi3</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.qwen.html">transformer_lens.pretrained.weight_conversions.qwen</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.qwen2.html">transformer_lens.pretrained.weight_conversions.qwen2</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.qwen3.html">transformer_lens.pretrained.weight_conversions.qwen3</a></li>
<li class="toctree-l5"><a class="reference internal" href="transformer_lens.pretrained.weight_conversions.t5.html">transformer_lens.pretrained.weight_conversions.t5</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="transformer_lens.utilities.html">transformer_lens.utilities</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.utilities.activation_functions.html">transformer_lens.utilities.activation_functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.utilities.addmm.html">transformer_lens.utilities.addmm</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.utilities.attention.html">transformer_lens.utilities.attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_lens.utilities.devices.html">transformer_lens.utilities.devices</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_properties_table.html">Model Properties Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../content/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/citation.html">Citation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos/Main_Demo.html">Transformer Lens Main Demo Notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos/Main_Demo.html#Setup">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos/Main_Demo.html#Introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos/Main_Demo.html#Features">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos/Exploratory_Analysis_Demo.html">Exploratory Analysis Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/special_cases.html">Special Cases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">News</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../content/news/release-2.0.html">TransformerLens 2.0</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../content/contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://transformerlensorg.github.io/TransformerLens/_static/coverage/">Code Coverage</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/TransformerLensOrg/TransformerLens">Github</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="module-transformer_lens.loading_from_pretrained">
<span id="transformer-lens-loading-from-pretrained"></span><h1>transformer_lens.loading_from_pretrained<a class="headerlink" href="#module-transformer_lens.loading_from_pretrained" title="Permalink to this heading">#</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">Config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'bool'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_norm_eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'float'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_vocab</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">50257</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'float'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.02</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_mlp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3072</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_head">
<span class="sig-name descname"><span class="pre">d_head</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">64</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_head" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_mlp">
<span class="sig-name descname"><span class="pre">d_mlp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">3072</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_mlp" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_model">
<span class="sig-name descname"><span class="pre">d_model</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">768</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_model" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_vocab">
<span class="sig-name descname"><span class="pre">d_vocab</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">50257</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_vocab" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.debug">
<span class="sig-name descname"><span class="pre">debug</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.debug" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.init_range">
<span class="sig-name descname"><span class="pre">init_range</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.02</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.init_range" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.layer_norm_eps">
<span class="sig-name descname"><span class="pre">layer_norm_eps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1e-05</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.layer_norm_eps" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.n_ctx">
<span class="sig-name descname"><span class="pre">n_ctx</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1024</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.n_ctx" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.n_heads">
<span class="sig-name descname"><span class="pre">n_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">12</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.n_heads" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.n_layers">
<span class="sig-name descname"><span class="pre">n_layers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">12</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.n_layers" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.MODEL_ALIASES">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">MODEL_ALIASES</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'01-ai/Yi-34B':</span> <span class="pre">['yi-34b',</span> <span class="pre">'Yi-34B'],</span> <span class="pre">'01-ai/Yi-34B-Chat':</span> <span class="pre">['yi-34b-chat',</span> <span class="pre">'Yi-34B-Chat'],</span> <span class="pre">'01-ai/Yi-6B':</span> <span class="pre">['yi-6b',</span> <span class="pre">'Yi-6B'],</span> <span class="pre">'01-ai/Yi-6B-Chat':</span> <span class="pre">['yi-6b-chat',</span> <span class="pre">'Yi-6B-Chat'],</span> <span class="pre">'ArthurConmy/redwood_attn_2l':</span> <span class="pre">['redwood_attn_2l'],</span> <span class="pre">'Baidicoot/Othello-GPT-Transformer-Lens':</span> <span class="pre">['othello-gpt'],</span> <span class="pre">'EleutherAI/gpt-j-6B':</span> <span class="pre">['gpt-j-6B',</span> <span class="pre">'gpt-j',</span> <span class="pre">'gptj'],</span> <span class="pre">'EleutherAI/gpt-neo-1.3B':</span> <span class="pre">['gpt-neo-1.3B',</span> <span class="pre">'gpt-neo-medium',</span> <span class="pre">'neo-medium'],</span> <span class="pre">'EleutherAI/gpt-neo-125M':</span> <span class="pre">['gpt-neo-125M',</span> <span class="pre">'gpt-neo-small',</span> <span class="pre">'neo-small',</span> <span class="pre">'neo'],</span> <span class="pre">'EleutherAI/gpt-neo-2.7B':</span> <span class="pre">['gpt-neo-2.7B',</span> <span class="pre">'gpt-neo-large',</span> <span class="pre">'neo-large'],</span> <span class="pre">'EleutherAI/gpt-neox-20b':</span> <span class="pre">['gpt-neox-20b',</span> <span class="pre">'gpt-neox',</span> <span class="pre">'neox'],</span> <span class="pre">'EleutherAI/pythia-1.4b':</span> <span class="pre">['pythia-1.4b',</span> <span class="pre">'EleutherAI/pythia-1.3b',</span> <span class="pre">'pythia-1.3b'],</span> <span class="pre">'EleutherAI/pythia-1.4b-deduped':</span> <span class="pre">['pythia-1.4b-deduped',</span> <span class="pre">'EleutherAI/pythia-1.3b-deduped',</span> <span class="pre">'pythia-1.3b-deduped'],</span> <span class="pre">'EleutherAI/pythia-1.4b-deduped-v0':</span> <span class="pre">['pythia-1.4b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-1.3b-deduped-v0',</span> <span class="pre">'pythia-1.3b-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-1.4b-v0':</span> <span class="pre">['pythia-1.4b-v0',</span> <span class="pre">'EleutherAI/pythia-1.3b-v0',</span> <span class="pre">'pythia-1.3b-v0'],</span> <span class="pre">'EleutherAI/pythia-12b':</span> <span class="pre">['pythia-12b',</span> <span class="pre">'EleutherAI/pythia-13b',</span> <span class="pre">'pythia-13b'],</span> <span class="pre">'EleutherAI/pythia-12b-deduped':</span> <span class="pre">['pythia-12b-deduped',</span> <span class="pre">'EleutherAI/pythia-13b-deduped',</span> <span class="pre">'pythia-13b-deduped'],</span> <span class="pre">'EleutherAI/pythia-12b-deduped-v0':</span> <span class="pre">['pythia-12b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-13b-deduped-v0',</span> <span class="pre">'pythia-13b-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-12b-v0':</span> <span class="pre">['pythia-12b-v0',</span> <span class="pre">'EleutherAI/pythia-13b-v0',</span> <span class="pre">'pythia-13b-v0'],</span> <span class="pre">'EleutherAI/pythia-14m':</span> <span class="pre">['pythia-14m'],</span> <span class="pre">'EleutherAI/pythia-160m':</span> <span class="pre">['pythia-160m',</span> <span class="pre">'EleutherAI/pythia-125m',</span> <span class="pre">'pythia-125m'],</span> <span class="pre">'EleutherAI/pythia-160m-deduped':</span> <span class="pre">['pythia-160m-deduped',</span> <span class="pre">'EleutherAI/pythia-125m-deduped',</span> <span class="pre">'pythia-125m-deduped'],</span> <span class="pre">'EleutherAI/pythia-160m-deduped-v0':</span> <span class="pre">['pythia-160m-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-125m-deduped-v0',</span> <span class="pre">'pythia-125m-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-160m-seed1':</span> <span class="pre">['pythia-160m-seed1',</span> <span class="pre">'EleutherAI/pythia-125m-seed1',</span> <span class="pre">'pythia-125m-seed1'],</span> <span class="pre">'EleutherAI/pythia-160m-seed2':</span> <span class="pre">['pythia-160m-seed2',</span> <span class="pre">'EleutherAI/pythia-125m-seed2',</span> <span class="pre">'pythia-125m-seed2'],</span> <span class="pre">'EleutherAI/pythia-160m-seed3':</span> <span class="pre">['pythia-160m-seed3',</span> <span class="pre">'EleutherAI/pythia-125m-seed3',</span> <span class="pre">'pythia-125m-seed3'],</span> <span class="pre">'EleutherAI/pythia-160m-v0':</span> <span class="pre">['pythia-160m-v0',</span> <span class="pre">'EleutherAI/pythia-125m-v0',</span> <span class="pre">'pythia-125m-v0'],</span> <span class="pre">'EleutherAI/pythia-1b':</span> <span class="pre">['pythia-1b',</span> <span class="pre">'EleutherAI/pythia-800m',</span> <span class="pre">'pythia-800m'],</span> <span class="pre">'EleutherAI/pythia-1b-deduped':</span> <span class="pre">['pythia-1b-deduped',</span> <span class="pre">'EleutherAI/pythia-800m-deduped',</span> <span class="pre">'pythia-800m-deduped'],</span> <span class="pre">'EleutherAI/pythia-1b-deduped-v0':</span> <span class="pre">['pythia-1b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-800m-deduped-v0',</span> <span class="pre">'pythia-800m-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-1b-v0':</span> <span class="pre">['pythia-1b-v0',</span> <span class="pre">'EleutherAI/pythia-800m-v0',</span> <span class="pre">'pythia-800m-v0'],</span> <span class="pre">'EleutherAI/pythia-2.8b':</span> <span class="pre">['pythia-2.8b',</span> <span class="pre">'EleutherAI/pythia-2.7b',</span> <span class="pre">'pythia-2.7b'],</span> <span class="pre">'EleutherAI/pythia-2.8b-deduped':</span> <span class="pre">['pythia-2.8b-deduped',</span> <span class="pre">'EleutherAI/pythia-2.7b-deduped',</span> <span class="pre">'pythia-2.7b-deduped'],</span> <span class="pre">'EleutherAI/pythia-2.8b-deduped-v0':</span> <span class="pre">['pythia-2.8b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-2.7b-deduped-v0',</span> <span class="pre">'pythia-2.7b-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-2.8b-v0':</span> <span class="pre">['pythia-2.8b-v0',</span> <span class="pre">'EleutherAI/pythia-2.7b-v0',</span> <span class="pre">'pythia-2.7b-v0'],</span> <span class="pre">'EleutherAI/pythia-31m':</span> <span class="pre">['pythia-31m'],</span> <span class="pre">'EleutherAI/pythia-410m':</span> <span class="pre">['pythia-410m',</span> <span class="pre">'EleutherAI/pythia-350m',</span> <span class="pre">'pythia-350m'],</span> <span class="pre">'EleutherAI/pythia-410m-deduped':</span> <span class="pre">['pythia-410m-deduped',</span> <span class="pre">'EleutherAI/pythia-350m-deduped',</span> <span class="pre">'pythia-350m-deduped'],</span> <span class="pre">'EleutherAI/pythia-410m-deduped-v0':</span> <span class="pre">['pythia-410m-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-350m-deduped-v0',</span> <span class="pre">'pythia-350m-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-410m-v0':</span> <span class="pre">['pythia-410m-v0',</span> <span class="pre">'EleutherAI/pythia-350m-v0',</span> <span class="pre">'pythia-350m-v0'],</span> <span class="pre">'EleutherAI/pythia-6.9b':</span> <span class="pre">['pythia-6.9b',</span> <span class="pre">'EleutherAI/pythia-6.7b',</span> <span class="pre">'pythia-6.7b'],</span> <span class="pre">'EleutherAI/pythia-6.9b-deduped':</span> <span class="pre">['pythia-6.9b-deduped',</span> <span class="pre">'EleutherAI/pythia-6.7b-deduped',</span> <span class="pre">'pythia-6.7b-deduped'],</span> <span class="pre">'EleutherAI/pythia-6.9b-deduped-v0':</span> <span class="pre">['pythia-6.9b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-6.7b-deduped-v0',</span> <span class="pre">'pythia-6.7b-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-6.9b-v0':</span> <span class="pre">['pythia-6.9b-v0',</span> <span class="pre">'EleutherAI/pythia-6.7b-v0',</span> <span class="pre">'pythia-6.7b-v0'],</span> <span class="pre">'EleutherAI/pythia-70m':</span> <span class="pre">['pythia-70m',</span> <span class="pre">'pythia',</span> <span class="pre">'EleutherAI/pythia-19m',</span> <span class="pre">'pythia-19m'],</span> <span class="pre">'EleutherAI/pythia-70m-deduped':</span> <span class="pre">['pythia-70m-deduped',</span> <span class="pre">'EleutherAI/pythia-19m-deduped',</span> <span class="pre">'pythia-19m-deduped'],</span> <span class="pre">'EleutherAI/pythia-70m-deduped-v0':</span> <span class="pre">['pythia-70m-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-19m-deduped-v0',</span> <span class="pre">'pythia-19m-deduped-v0'],</span> <span class="pre">'EleutherAI/pythia-70m-v0':</span> <span class="pre">['pythia-70m-v0',</span> <span class="pre">'pythia-v0',</span> <span class="pre">'EleutherAI/pythia-19m-v0',</span> <span class="pre">'pythia-19m-v0'],</span> <span class="pre">'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr':</span> <span class="pre">['attn-only-2l-demo',</span> <span class="pre">'attn-only-2l-shortformer-6b-big-lr',</span> <span class="pre">'attn-only-2l-induction-demo',</span> <span class="pre">'attn-only-demo'],</span> <span class="pre">'NeelNanda/Attn_Only_1L512W_C4_Code':</span> <span class="pre">['attn-only-1l',</span> <span class="pre">'attn-only-1l-new',</span> <span class="pre">'attn-only-1l-c4-code'],</span> <span class="pre">'NeelNanda/Attn_Only_2L512W_C4_Code':</span> <span class="pre">['attn-only-2l',</span> <span class="pre">'attn-only-2l-new',</span> <span class="pre">'attn-only-2l-c4-code'],</span> <span class="pre">'NeelNanda/Attn_Only_3L512W_C4_Code':</span> <span class="pre">['attn-only-3l',</span> <span class="pre">'attn-only-3l-new',</span> <span class="pre">'attn-only-3l-c4-code'],</span> <span class="pre">'NeelNanda/Attn_Only_4L512W_C4_Code':</span> <span class="pre">['attn-only-4l',</span> <span class="pre">'attn-only-4l-new',</span> <span class="pre">'attn-only-4l-c4-code'],</span> <span class="pre">'NeelNanda/GELU_1L512W_C4_Code':</span> <span class="pre">['gelu-1l',</span> <span class="pre">'gelu-1l-new',</span> <span class="pre">'gelu-1l-c4-code'],</span> <span class="pre">'NeelNanda/GELU_2L512W_C4_Code':</span> <span class="pre">['gelu-2l',</span> <span class="pre">'gelu-2l-new',</span> <span class="pre">'gelu-2l-c4-code'],</span> <span class="pre">'NeelNanda/GELU_3L512W_C4_Code':</span> <span class="pre">['gelu-3l',</span> <span class="pre">'gelu-3l-new',</span> <span class="pre">'gelu-3l-c4-code'],</span> <span class="pre">'NeelNanda/GELU_4L512W_C4_Code':</span> <span class="pre">['gelu-4l',</span> <span class="pre">'gelu-4l-new',</span> <span class="pre">'gelu-4l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_10L1280W_C4_Code':</span> <span class="pre">['solu-10l',</span> <span class="pre">'solu-10l-new',</span> <span class="pre">'solu-10l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_10L_v22_old':</span> <span class="pre">['solu-10l-pile',</span> <span class="pre">'solu-10l-old'],</span> <span class="pre">'NeelNanda/SoLU_12L1536W_C4_Code':</span> <span class="pre">['solu-12l',</span> <span class="pre">'solu-12l-new',</span> <span class="pre">'solu-12l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_12L_v23_old':</span> <span class="pre">['solu-12l-pile',</span> <span class="pre">'solu-12l-old'],</span> <span class="pre">'NeelNanda/SoLU_1L512W_C4_Code':</span> <span class="pre">['solu-1l',</span> <span class="pre">'solu-1l-new',</span> <span class="pre">'solu-1l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_1L512W_Wiki_Finetune':</span> <span class="pre">['solu-1l-wiki',</span> <span class="pre">'solu-1l-wiki-finetune',</span> <span class="pre">'solu-1l-finetune'],</span> <span class="pre">'NeelNanda/SoLU_1L_v9_old':</span> <span class="pre">['solu-1l-pile',</span> <span class="pre">'solu-1l-old'],</span> <span class="pre">'NeelNanda/SoLU_2L512W_C4_Code':</span> <span class="pre">['solu-2l',</span> <span class="pre">'solu-2l-new',</span> <span class="pre">'solu-2l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_2L_v10_old':</span> <span class="pre">['solu-2l-pile',</span> <span class="pre">'solu-2l-old'],</span> <span class="pre">'NeelNanda/SoLU_3L512W_C4_Code':</span> <span class="pre">['solu-3l',</span> <span class="pre">'solu-3l-new',</span> <span class="pre">'solu-3l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_4L512W_C4_Code':</span> <span class="pre">['solu-4l',</span> <span class="pre">'solu-4l-new',</span> <span class="pre">'solu-4l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_4L512W_Wiki_Finetune':</span> <span class="pre">['solu-4l-wiki',</span> <span class="pre">'solu-4l-wiki-finetune',</span> <span class="pre">'solu-4l-finetune'],</span> <span class="pre">'NeelNanda/SoLU_4L_v11_old':</span> <span class="pre">['solu-4l-pile',</span> <span class="pre">'solu-4l-old'],</span> <span class="pre">'NeelNanda/SoLU_6L768W_C4_Code':</span> <span class="pre">['solu-6l',</span> <span class="pre">'solu-6l-new',</span> <span class="pre">'solu-6l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_6L_v13_old':</span> <span class="pre">['solu-6l-pile',</span> <span class="pre">'solu-6l-old'],</span> <span class="pre">'NeelNanda/SoLU_8L1024W_C4_Code':</span> <span class="pre">['solu-8l',</span> <span class="pre">'solu-8l-new',</span> <span class="pre">'solu-8l-c4-code'],</span> <span class="pre">'NeelNanda/SoLU_8L_v21_old':</span> <span class="pre">['solu-8l-pile',</span> <span class="pre">'solu-8l-old'],</span> <span class="pre">'Qwen/QwQ-32B-Preview':</span> <span class="pre">['qwen-32b-preview'],</span> <span class="pre">'Qwen/Qwen-14B':</span> <span class="pre">['qwen-14b'],</span> <span class="pre">'Qwen/Qwen-14B-Chat':</span> <span class="pre">['qwen-14b-chat'],</span> <span class="pre">'Qwen/Qwen-1_8B':</span> <span class="pre">['qwen-1.8b'],</span> <span class="pre">'Qwen/Qwen-1_8B-Chat':</span> <span class="pre">['qwen-1.8b-chat'],</span> <span class="pre">'Qwen/Qwen-7B':</span> <span class="pre">['qwen-7b'],</span> <span class="pre">'Qwen/Qwen-7B-Chat':</span> <span class="pre">['qwen-7b-chat'],</span> <span class="pre">'Qwen/Qwen1.5-0.5B':</span> <span class="pre">['qwen1.5-0.5b'],</span> <span class="pre">'Qwen/Qwen1.5-0.5B-Chat':</span> <span class="pre">['qwen1.5-0.5b-chat'],</span> <span class="pre">'Qwen/Qwen1.5-1.8B':</span> <span class="pre">['qwen1.5-1.8b'],</span> <span class="pre">'Qwen/Qwen1.5-1.8B-Chat':</span> <span class="pre">['qwen1.5-1.8b-chat'],</span> <span class="pre">'Qwen/Qwen1.5-14B':</span> <span class="pre">['qwen1.5-14b'],</span> <span class="pre">'Qwen/Qwen1.5-14B-Chat':</span> <span class="pre">['qwen1.5-14b-chat'],</span> <span class="pre">'Qwen/Qwen1.5-4B':</span> <span class="pre">['qwen1.5-4b'],</span> <span class="pre">'Qwen/Qwen1.5-4B-Chat':</span> <span class="pre">['qwen1.5-4b-chat'],</span> <span class="pre">'Qwen/Qwen1.5-7B':</span> <span class="pre">['qwen1.5-7b'],</span> <span class="pre">'Qwen/Qwen1.5-7B-Chat':</span> <span class="pre">['qwen1.5-7b-chat'],</span> <span class="pre">'Qwen/Qwen2-0.5B':</span> <span class="pre">['qwen2-0.5b'],</span> <span class="pre">'Qwen/Qwen2-0.5B-Instruct':</span> <span class="pre">['qwen2-0.5b-instruct'],</span> <span class="pre">'Qwen/Qwen2-1.5B':</span> <span class="pre">['qwen2-1.5b'],</span> <span class="pre">'Qwen/Qwen2-1.5B-Instruct':</span> <span class="pre">['qwen2-1.5b-instruct'],</span> <span class="pre">'Qwen/Qwen2-7B':</span> <span class="pre">['qwen2-7b'],</span> <span class="pre">'Qwen/Qwen2-7B-Instruct':</span> <span class="pre">['qwen2-7b-instruct'],</span> <span class="pre">'Qwen/Qwen2.5-0.5B':</span> <span class="pre">['qwen2.5-0.5b'],</span> <span class="pre">'Qwen/Qwen2.5-0.5B-Instruct':</span> <span class="pre">['qwen2.5-0.5b-instruct'],</span> <span class="pre">'Qwen/Qwen2.5-1.5B':</span> <span class="pre">['qwen2.5-1.5b'],</span> <span class="pre">'Qwen/Qwen2.5-1.5B-Instruct':</span> <span class="pre">['qwen2.5-1.5b-instruct'],</span> <span class="pre">'Qwen/Qwen2.5-14B':</span> <span class="pre">['qwen2.5-14b'],</span> <span class="pre">'Qwen/Qwen2.5-14B-Instruct':</span> <span class="pre">['qwen2.5-14b-instruct'],</span> <span class="pre">'Qwen/Qwen2.5-32B':</span> <span class="pre">['qwen2.5-32b'],</span> <span class="pre">'Qwen/Qwen2.5-32B-Instruct':</span> <span class="pre">['qwen2.5-32b-instruct'],</span> <span class="pre">'Qwen/Qwen2.5-3B':</span> <span class="pre">['qwen2.5-3b'],</span> <span class="pre">'Qwen/Qwen2.5-3B-Instruct':</span> <span class="pre">['qwen2.5-3b-instruct'],</span> <span class="pre">'Qwen/Qwen2.5-72B':</span> <span class="pre">['qwen2.5-72b'],</span> <span class="pre">'Qwen/Qwen2.5-72B-Instruct':</span> <span class="pre">['qwen2.5-72b-instruct'],</span> <span class="pre">'Qwen/Qwen2.5-7B':</span> <span class="pre">['qwen2.5-7b'],</span> <span class="pre">'Qwen/Qwen2.5-7B-Instruct':</span> <span class="pre">['qwen2.5-7b-instruct'],</span> <span class="pre">'Qwen/Qwen3-0.6B':</span> <span class="pre">['qwen3-0.6b'],</span> <span class="pre">'Qwen/Qwen3-1.7B':</span> <span class="pre">['qwen3-1.7b'],</span> <span class="pre">'Qwen/Qwen3-14B':</span> <span class="pre">['qwen3-14b'],</span> <span class="pre">'Qwen/Qwen3-4B':</span> <span class="pre">['qwen3-4b'],</span> <span class="pre">'Qwen/Qwen3-8B':</span> <span class="pre">['qwen3-8b'],</span> <span class="pre">'ai-forever/mGPT':</span> <span class="pre">['mGPT'],</span> <span class="pre">'bigcode/santacoder':</span> <span class="pre">['santacoder'],</span> <span class="pre">'bigscience/bloom-1b1':</span> <span class="pre">['bloom-1b1'],</span> <span class="pre">'bigscience/bloom-1b7':</span> <span class="pre">['bloom-1b7'],</span> <span class="pre">'bigscience/bloom-3b':</span> <span class="pre">['bloom-3b'],</span> <span class="pre">'bigscience/bloom-560m':</span> <span class="pre">['bloom-560m'],</span> <span class="pre">'bigscience/bloom-7b1':</span> <span class="pre">['bloom-7b1'],</span> <span class="pre">'codellama/CodeLlama-7b-Instruct-hf':</span> <span class="pre">['CodeLlama-7b-instruct',</span> <span class="pre">'codellama/CodeLlama-7b-Instruct-hf'],</span> <span class="pre">'codellama/CodeLlama-7b-Python-hf':</span> <span class="pre">['CodeLlama-7b-python',</span> <span class="pre">'codellama/CodeLlama-7b-Python-hf'],</span> <span class="pre">'codellama/CodeLlama-7b-hf':</span> <span class="pre">['CodeLlamallama-2-7b',</span> <span class="pre">'codellama/CodeLlama-7b-hf'],</span> <span class="pre">'distilgpt2':</span> <span class="pre">['distillgpt2',</span> <span class="pre">'distill-gpt2',</span> <span class="pre">'distil-gpt2',</span> <span class="pre">'gpt2-xs'],</span> <span class="pre">'facebook/opt-1.3b':</span> <span class="pre">['opt-1.3b',</span> <span class="pre">'opt-medium'],</span> <span class="pre">'facebook/opt-125m':</span> <span class="pre">['opt-125m',</span> <span class="pre">'opt-small',</span> <span class="pre">'opt'],</span> <span class="pre">'facebook/opt-13b':</span> <span class="pre">['opt-13b',</span> <span class="pre">'opt-xxl'],</span> <span class="pre">'facebook/opt-2.7b':</span> <span class="pre">['opt-2.7b',</span> <span class="pre">'opt-large'],</span> <span class="pre">'facebook/opt-30b':</span> <span class="pre">['opt-30b',</span> <span class="pre">'opt-xxxl'],</span> <span class="pre">'facebook/opt-6.7b':</span> <span class="pre">['opt-6.7b',</span> <span class="pre">'opt-xl'],</span> <span class="pre">'facebook/opt-66b':</span> <span class="pre">['opt-66b',</span> <span class="pre">'opt-xxxxl'],</span> <span class="pre">'google-bert/bert-base-cased':</span> <span class="pre">['bert-base-cased'],</span> <span class="pre">'google-bert/bert-base-uncased':</span> <span class="pre">['bert-base-uncased'],</span> <span class="pre">'google-bert/bert-large-cased':</span> <span class="pre">['bert-large-cased'],</span> <span class="pre">'google-bert/bert-large-uncased':</span> <span class="pre">['bert-large-uncased'],</span> <span class="pre">'google-t5/t5-base':</span> <span class="pre">['t5-base'],</span> <span class="pre">'google-t5/t5-large':</span> <span class="pre">['t5-large'],</span> <span class="pre">'google-t5/t5-small':</span> <span class="pre">['t5-small'],</span> <span class="pre">'google/gemma-2-27b':</span> <span class="pre">['gemma-2-27b'],</span> <span class="pre">'google/gemma-2-27b-it':</span> <span class="pre">['gemma-2-27b-it'],</span> <span class="pre">'google/gemma-2-2b':</span> <span class="pre">['gemma-2-2b'],</span> <span class="pre">'google/gemma-2-2b-it':</span> <span class="pre">['gemma-2-2b-it'],</span> <span class="pre">'google/gemma-2-9b':</span> <span class="pre">['gemma-2-9b'],</span> <span class="pre">'google/gemma-2-9b-it':</span> <span class="pre">['gemma-2-9b-it'],</span> <span class="pre">'google/gemma-2b':</span> <span class="pre">['gemma-2b'],</span> <span class="pre">'google/gemma-2b-it':</span> <span class="pre">['gemma-2b-it'],</span> <span class="pre">'google/gemma-7b':</span> <span class="pre">['gemma-7b'],</span> <span class="pre">'google/gemma-7b-it':</span> <span class="pre">['gemma-7b-it'],</span> <span class="pre">'gpt2':</span> <span class="pre">['gpt2-small'],</span> <span class="pre">'llama-13b-hf':</span> <span class="pre">['llama-13b'],</span> <span class="pre">'llama-30b-hf':</span> <span class="pre">['llama-30b'],</span> <span class="pre">'llama-65b-hf':</span> <span class="pre">['llama-65b'],</span> <span class="pre">'llama-7b-hf':</span> <span class="pre">['llama-7b'],</span> <span class="pre">'meta-llama/Llama-2-13b-chat-hf':</span> <span class="pre">['Llama-2-13b-chat',</span> <span class="pre">'meta-llama/Llama-2-13b-chat-hf'],</span> <span class="pre">'meta-llama/Llama-2-13b-hf':</span> <span class="pre">['Llama-2-13b',</span> <span class="pre">'meta-llama/Llama-2-13b-hf'],</span> <span class="pre">'meta-llama/Llama-2-70b-chat-hf':</span> <span class="pre">['Llama-2-70b-chat',</span> <span class="pre">'meta-llama-2-70b-chat-hf'],</span> <span class="pre">'meta-llama/Llama-2-7b-chat-hf':</span> <span class="pre">['Llama-2-7b-chat',</span> <span class="pre">'meta-llama/Llama-2-7b-chat-hf'],</span> <span class="pre">'meta-llama/Llama-2-7b-hf':</span> <span class="pre">['Llama-2-7b',</span> <span class="pre">'meta-llama/Llama-2-7b-hf'],</span> <span class="pre">'microsoft/Phi-3-mini-4k-instruct':</span> <span class="pre">['phi-3'],</span> <span class="pre">'microsoft/phi-1':</span> <span class="pre">['phi-1'],</span> <span class="pre">'microsoft/phi-1_5':</span> <span class="pre">['phi-1_5'],</span> <span class="pre">'microsoft/phi-2':</span> <span class="pre">['phi-2'],</span> <span class="pre">'microsoft/phi-4':</span> <span class="pre">['phi-4'],</span> <span class="pre">'mistralai/Mistral-7B-Instruct-v0.1':</span> <span class="pre">['mistral-7b-instruct'],</span> <span class="pre">'mistralai/Mistral-7B-v0.1':</span> <span class="pre">['mistral-7b'],</span> <span class="pre">'mistralai/Mistral-Nemo-Base-2407':</span> <span class="pre">['mistral-nemo-base-2407'],</span> <span class="pre">'mistralai/Mixtral-8x7B-Instruct-v0.1':</span> <span class="pre">['mixtral-instruct',</span> <span class="pre">'mixtral-8x7b-instruct'],</span> <span class="pre">'mistralai/Mixtral-8x7B-v0.1':</span> <span class="pre">['mixtral',</span> <span class="pre">'mixtral-8x7b'],</span> <span class="pre">'roneneldan/TinyStories-1Layer-21M':</span> <span class="pre">['tiny-stories-1L-21M'],</span> <span class="pre">'roneneldan/TinyStories-1M':</span> <span class="pre">['tiny-stories-1M'],</span> <span class="pre">'roneneldan/TinyStories-28M':</span> <span class="pre">['tiny-stories-28M'],</span> <span class="pre">'roneneldan/TinyStories-2Layers-33M':</span> <span class="pre">['tiny-stories-2L-33M'],</span> <span class="pre">'roneneldan/TinyStories-33M':</span> <span class="pre">['tiny-stories-33M'],</span> <span class="pre">'roneneldan/TinyStories-3M':</span> <span class="pre">['tiny-stories-3M'],</span> <span class="pre">'roneneldan/TinyStories-8M':</span> <span class="pre">['tiny-stories-8M'],</span> <span class="pre">'roneneldan/TinyStories-Instruct-1M':</span> <span class="pre">['tiny-stories-instruct-1M'],</span> <span class="pre">'roneneldan/TinyStories-Instruct-28M':</span> <span class="pre">['tiny-stories-instruct-28M'],</span> <span class="pre">'roneneldan/TinyStories-Instruct-2Layers-33M':</span> <span class="pre">['tiny-stories-instruct-2L-33M'],</span> <span class="pre">'roneneldan/TinyStories-Instruct-33M':</span> <span class="pre">['tiny-stories-instruct-33M'],</span> <span class="pre">'roneneldan/TinyStories-Instruct-3M':</span> <span class="pre">['tiny-stories-instruct-3M'],</span> <span class="pre">'roneneldan/TinyStories-Instruct-8M':</span> <span class="pre">['tiny-stories-instruct-8M'],</span> <span class="pre">'roneneldan/TinyStories-Instuct-1Layer-21M':</span> <span class="pre">['tiny-stories-instruct-1L-21M'],</span> <span class="pre">'stabilityai/stablelm-base-alpha-3b':</span> <span class="pre">['stablelm-base-alpha-3b',</span> <span class="pre">'stablelm-base-3b'],</span> <span class="pre">'stabilityai/stablelm-base-alpha-7b':</span> <span class="pre">['stablelm-base-alpha-7b',</span> <span class="pre">'stablelm-base-7b'],</span> <span class="pre">'stabilityai/stablelm-tuned-alpha-3b':</span> <span class="pre">['stablelm-tuned-alpha-3b',</span> <span class="pre">'stablelm-tuned-3b'],</span> <span class="pre">'stabilityai/stablelm-tuned-alpha-7b':</span> <span class="pre">['stablelm-tuned-alpha-7b',</span> <span class="pre">'stablelm-tuned-7b'],</span> <span class="pre">'stanford-crfm/alias-gpt2-small-x21':</span> <span class="pre">['stanford-gpt2-small-a',</span> <span class="pre">'alias-gpt2-small-x21',</span> <span class="pre">'gpt2-mistral-small-a',</span> <span class="pre">'gpt2-stanford-small-a'],</span> <span class="pre">'stanford-crfm/arwen-gpt2-medium-x21':</span> <span class="pre">['stanford-gpt2-medium-a',</span> <span class="pre">'arwen-gpt2-medium-x21',</span> <span class="pre">'gpt2-medium-small-a',</span> <span class="pre">'gpt2-stanford-medium-a'],</span> <span class="pre">'stanford-crfm/battlestar-gpt2-small-x49':</span> <span class="pre">['stanford-gpt2-small-b',</span> <span class="pre">'battlestar-gpt2-small-x49',</span> <span class="pre">'gpt2-mistral-small-b',</span> <span class="pre">'gpt2-mistral-small-b'],</span> <span class="pre">'stanford-crfm/beren-gpt2-medium-x49':</span> <span class="pre">['stanford-gpt2-medium-b',</span> <span class="pre">'beren-gpt2-medium-x49',</span> <span class="pre">'gpt2-medium-small-b',</span> <span class="pre">'gpt2-stanford-medium-b'],</span> <span class="pre">'stanford-crfm/caprica-gpt2-small-x81':</span> <span class="pre">['stanford-gpt2-small-c',</span> <span class="pre">'caprica-gpt2-small-x81',</span> <span class="pre">'gpt2-mistral-small-c',</span> <span class="pre">'gpt2-stanford-small-c'],</span> <span class="pre">'stanford-crfm/celebrimbor-gpt2-medium-x81':</span> <span class="pre">['stanford-gpt2-medium-c',</span> <span class="pre">'celebrimbor-gpt2-medium-x81',</span> <span class="pre">'gpt2-medium-small-c',</span> <span class="pre">'gpt2-medium-small-c'],</span> <span class="pre">'stanford-crfm/darkmatter-gpt2-small-x343':</span> <span class="pre">['stanford-gpt2-small-d',</span> <span class="pre">'darkmatter-gpt2-small-x343',</span> <span class="pre">'gpt2-mistral-small-d',</span> <span class="pre">'gpt2-mistral-small-d'],</span> <span class="pre">'stanford-crfm/durin-gpt2-medium-x343':</span> <span class="pre">['stanford-gpt2-medium-d',</span> <span class="pre">'durin-gpt2-medium-x343',</span> <span class="pre">'gpt2-medium-small-d',</span> <span class="pre">'gpt2-stanford-medium-d'],</span> <span class="pre">'stanford-crfm/eowyn-gpt2-medium-x777':</span> <span class="pre">['stanford-gpt2-medium-e',</span> <span class="pre">'eowyn-gpt2-medium-x777',</span> <span class="pre">'gpt2-medium-small-e',</span> <span class="pre">'gpt2-stanford-medium-e'],</span> <span class="pre">'stanford-crfm/expanse-gpt2-small-x777':</span> <span class="pre">['stanford-gpt2-small-e',</span> <span class="pre">'expanse-gpt2-small-x777',</span> <span class="pre">'gpt2-mistral-small-e',</span> <span class="pre">'gpt2-mistral-small-e']}</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.MODEL_ALIASES" title="Permalink to this definition">#</a></dt>
<dd><p>Model aliases for models on HuggingFace.</p>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.NON_HF_HOSTED_MODEL_NAMES">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">NON_HF_HOSTED_MODEL_NAMES</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['llama-7b-hf',</span> <span class="pre">'llama-13b-hf',</span> <span class="pre">'llama-30b-hf',</span> <span class="pre">'llama-65b-hf']</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.NON_HF_HOSTED_MODEL_NAMES" title="Permalink to this definition">#</a></dt>
<dd><p>Official model names for models not hosted on HuggingFace.</p>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">OFFICIAL_MODEL_NAMES</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['gpt2',</span> <span class="pre">'gpt2-medium',</span> <span class="pre">'gpt2-large',</span> <span class="pre">'gpt2-xl',</span> <span class="pre">'distilgpt2',</span> <span class="pre">'facebook/opt-125m',</span> <span class="pre">'facebook/opt-1.3b',</span> <span class="pre">'facebook/opt-2.7b',</span> <span class="pre">'facebook/opt-6.7b',</span> <span class="pre">'facebook/opt-13b',</span> <span class="pre">'facebook/opt-30b',</span> <span class="pre">'facebook/opt-66b',</span> <span class="pre">'EleutherAI/gpt-neo-125M',</span> <span class="pre">'EleutherAI/gpt-neo-1.3B',</span> <span class="pre">'EleutherAI/gpt-neo-2.7B',</span> <span class="pre">'EleutherAI/gpt-j-6B',</span> <span class="pre">'EleutherAI/gpt-neox-20b',</span> <span class="pre">'stanford-crfm/alias-gpt2-small-x21',</span> <span class="pre">'stanford-crfm/battlestar-gpt2-small-x49',</span> <span class="pre">'stanford-crfm/caprica-gpt2-small-x81',</span> <span class="pre">'stanford-crfm/darkmatter-gpt2-small-x343',</span> <span class="pre">'stanford-crfm/expanse-gpt2-small-x777',</span> <span class="pre">'stanford-crfm/arwen-gpt2-medium-x21',</span> <span class="pre">'stanford-crfm/beren-gpt2-medium-x49',</span> <span class="pre">'stanford-crfm/celebrimbor-gpt2-medium-x81',</span> <span class="pre">'stanford-crfm/durin-gpt2-medium-x343',</span> <span class="pre">'stanford-crfm/eowyn-gpt2-medium-x777',</span> <span class="pre">'EleutherAI/pythia-14m',</span> <span class="pre">'EleutherAI/pythia-31m',</span> <span class="pre">'EleutherAI/pythia-70m',</span> <span class="pre">'EleutherAI/pythia-160m',</span> <span class="pre">'EleutherAI/pythia-410m',</span> <span class="pre">'EleutherAI/pythia-1b',</span> <span class="pre">'EleutherAI/pythia-1.4b',</span> <span class="pre">'EleutherAI/pythia-2.8b',</span> <span class="pre">'EleutherAI/pythia-6.9b',</span> <span class="pre">'EleutherAI/pythia-12b',</span> <span class="pre">'EleutherAI/pythia-70m-deduped',</span> <span class="pre">'EleutherAI/pythia-160m-deduped',</span> <span class="pre">'EleutherAI/pythia-410m-deduped',</span> <span class="pre">'EleutherAI/pythia-1b-deduped',</span> <span class="pre">'EleutherAI/pythia-1.4b-deduped',</span> <span class="pre">'EleutherAI/pythia-2.8b-deduped',</span> <span class="pre">'EleutherAI/pythia-6.9b-deduped',</span> <span class="pre">'EleutherAI/pythia-12b-deduped',</span> <span class="pre">'EleutherAI/pythia-70m-v0',</span> <span class="pre">'EleutherAI/pythia-160m-v0',</span> <span class="pre">'EleutherAI/pythia-410m-v0',</span> <span class="pre">'EleutherAI/pythia-1b-v0',</span> <span class="pre">'EleutherAI/pythia-1.4b-v0',</span> <span class="pre">'EleutherAI/pythia-2.8b-v0',</span> <span class="pre">'EleutherAI/pythia-6.9b-v0',</span> <span class="pre">'EleutherAI/pythia-12b-v0',</span> <span class="pre">'EleutherAI/pythia-70m-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-160m-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-410m-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-1b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-1.4b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-2.8b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-6.9b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-12b-deduped-v0',</span> <span class="pre">'EleutherAI/pythia-160m-seed1',</span> <span class="pre">'EleutherAI/pythia-160m-seed2',</span> <span class="pre">'EleutherAI/pythia-160m-seed3',</span> <span class="pre">'NeelNanda/SoLU_1L_v9_old',</span> <span class="pre">'NeelNanda/SoLU_2L_v10_old',</span> <span class="pre">'NeelNanda/SoLU_4L_v11_old',</span> <span class="pre">'NeelNanda/SoLU_6L_v13_old',</span> <span class="pre">'NeelNanda/SoLU_8L_v21_old',</span> <span class="pre">'NeelNanda/SoLU_10L_v22_old',</span> <span class="pre">'NeelNanda/SoLU_12L_v23_old',</span> <span class="pre">'NeelNanda/SoLU_1L512W_C4_Code',</span> <span class="pre">'NeelNanda/SoLU_2L512W_C4_Code',</span> <span class="pre">'NeelNanda/SoLU_3L512W_C4_Code',</span> <span class="pre">'NeelNanda/SoLU_4L512W_C4_Code',</span> <span class="pre">'NeelNanda/SoLU_6L768W_C4_Code',</span> <span class="pre">'NeelNanda/SoLU_8L1024W_C4_Code',</span> <span class="pre">'NeelNanda/SoLU_10L1280W_C4_Code',</span> <span class="pre">'NeelNanda/SoLU_12L1536W_C4_Code',</span> <span class="pre">'NeelNanda/GELU_1L512W_C4_Code',</span> <span class="pre">'NeelNanda/GELU_2L512W_C4_Code',</span> <span class="pre">'NeelNanda/GELU_3L512W_C4_Code',</span> <span class="pre">'NeelNanda/GELU_4L512W_C4_Code',</span> <span class="pre">'NeelNanda/Attn_Only_1L512W_C4_Code',</span> <span class="pre">'NeelNanda/Attn_Only_2L512W_C4_Code',</span> <span class="pre">'NeelNanda/Attn_Only_3L512W_C4_Code',</span> <span class="pre">'NeelNanda/Attn_Only_4L512W_C4_Code',</span> <span class="pre">'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr',</span> <span class="pre">'NeelNanda/SoLU_1L512W_Wiki_Finetune',</span> <span class="pre">'NeelNanda/SoLU_4L512W_Wiki_Finetune',</span> <span class="pre">'ArthurConmy/redwood_attn_2l',</span> <span class="pre">'llama-7b-hf',</span> <span class="pre">'llama-13b-hf',</span> <span class="pre">'llama-30b-hf',</span> <span class="pre">'llama-65b-hf',</span> <span class="pre">'meta-llama/Llama-2-7b-hf',</span> <span class="pre">'meta-llama/Llama-2-7b-chat-hf',</span> <span class="pre">'meta-llama/Llama-2-13b-hf',</span> <span class="pre">'meta-llama/Llama-2-13b-chat-hf',</span> <span class="pre">'meta-llama/Llama-2-70b-chat-hf',</span> <span class="pre">'codellama/CodeLlama-7b-hf',</span> <span class="pre">'codellama/CodeLlama-7b-Python-hf',</span> <span class="pre">'codellama/CodeLlama-7b-Instruct-hf',</span> <span class="pre">'meta-llama/Meta-Llama-3-8B',</span> <span class="pre">'meta-llama/Meta-Llama-3-8B-Instruct',</span> <span class="pre">'meta-llama/Meta-Llama-3-70B',</span> <span class="pre">'meta-llama/Meta-Llama-3-70B-Instruct',</span> <span class="pre">'meta-llama/Llama-3.1-70B',</span> <span class="pre">'meta-llama/Llama-3.1-8B',</span> <span class="pre">'meta-llama/Llama-3.1-8B-Instruct',</span> <span class="pre">'meta-llama/Llama-3.1-70B-Instruct',</span> <span class="pre">'meta-llama/Llama-3.2-1B',</span> <span class="pre">'meta-llama/Llama-3.2-3B',</span> <span class="pre">'meta-llama/Llama-3.2-1B-Instruct',</span> <span class="pre">'meta-llama/Llama-3.2-3B-Instruct',</span> <span class="pre">'meta-llama/Llama-3.3-70B-Instruct',</span> <span class="pre">'Baidicoot/Othello-GPT-Transformer-Lens',</span> <span class="pre">'google-bert/bert-base-cased',</span> <span class="pre">'google-bert/bert-base-uncased',</span> <span class="pre">'google-bert/bert-large-cased',</span> <span class="pre">'google-bert/bert-large-uncased',</span> <span class="pre">'roneneldan/TinyStories-1M',</span> <span class="pre">'roneneldan/TinyStories-3M',</span> <span class="pre">'roneneldan/TinyStories-8M',</span> <span class="pre">'roneneldan/TinyStories-28M',</span> <span class="pre">'roneneldan/TinyStories-33M',</span> <span class="pre">'roneneldan/TinyStories-Instruct-1M',</span> <span class="pre">'roneneldan/TinyStories-Instruct-3M',</span> <span class="pre">'roneneldan/TinyStories-Instruct-8M',</span> <span class="pre">'roneneldan/TinyStories-Instruct-28M',</span> <span class="pre">'roneneldan/TinyStories-Instruct-33M',</span> <span class="pre">'roneneldan/TinyStories-1Layer-21M',</span> <span class="pre">'roneneldan/TinyStories-2Layers-33M',</span> <span class="pre">'roneneldan/TinyStories-Instuct-1Layer-21M',</span> <span class="pre">'roneneldan/TinyStories-Instruct-2Layers-33M',</span> <span class="pre">'stabilityai/stablelm-base-alpha-3b',</span> <span class="pre">'stabilityai/stablelm-base-alpha-7b',</span> <span class="pre">'stabilityai/stablelm-tuned-alpha-3b',</span> <span class="pre">'stabilityai/stablelm-tuned-alpha-7b',</span> <span class="pre">'mistralai/Mistral-7B-v0.1',</span> <span class="pre">'mistralai/Mistral-7B-Instruct-v0.1',</span> <span class="pre">'mistralai/Mistral-Small-24B-Base-2501',</span> <span class="pre">'mistralai/Mistral-Nemo-Base-2407',</span> <span class="pre">'mistralai/Mixtral-8x7B-v0.1',</span> <span class="pre">'mistralai/Mixtral-8x7B-Instruct-v0.1',</span> <span class="pre">'bigscience/bloom-560m',</span> <span class="pre">'bigscience/bloom-1b1',</span> <span class="pre">'bigscience/bloom-1b7',</span> <span class="pre">'bigscience/bloom-3b',</span> <span class="pre">'bigscience/bloom-7b1',</span> <span class="pre">'bigcode/santacoder',</span> <span class="pre">'Qwen/Qwen-1_8B',</span> <span class="pre">'Qwen/Qwen-7B',</span> <span class="pre">'Qwen/Qwen-14B',</span> <span class="pre">'Qwen/Qwen-1_8B-Chat',</span> <span class="pre">'Qwen/Qwen-7B-Chat',</span> <span class="pre">'Qwen/Qwen-14B-Chat',</span> <span class="pre">'Qwen/Qwen1.5-0.5B',</span> <span class="pre">'Qwen/Qwen1.5-0.5B-Chat',</span> <span class="pre">'Qwen/Qwen1.5-1.8B',</span> <span class="pre">'Qwen/Qwen1.5-1.8B-Chat',</span> <span class="pre">'Qwen/Qwen1.5-4B',</span> <span class="pre">'Qwen/Qwen1.5-4B-Chat',</span> <span class="pre">'Qwen/Qwen1.5-7B',</span> <span class="pre">'Qwen/Qwen1.5-7B-Chat',</span> <span class="pre">'Qwen/Qwen1.5-14B',</span> <span class="pre">'Qwen/Qwen1.5-14B-Chat',</span> <span class="pre">'Qwen/Qwen2-0.5B',</span> <span class="pre">'Qwen/Qwen2-0.5B-Instruct',</span> <span class="pre">'Qwen/Qwen2-1.5B',</span> <span class="pre">'Qwen/Qwen2-1.5B-Instruct',</span> <span class="pre">'Qwen/Qwen2-7B',</span> <span class="pre">'Qwen/Qwen2-7B-Instruct',</span> <span class="pre">'Qwen/Qwen2.5-0.5B',</span> <span class="pre">'Qwen/Qwen2.5-0.5B-Instruct',</span> <span class="pre">'Qwen/Qwen2.5-1.5B',</span> <span class="pre">'Qwen/Qwen2.5-1.5B-Instruct',</span> <span class="pre">'Qwen/Qwen2.5-3B',</span> <span class="pre">'Qwen/Qwen2.5-3B-Instruct',</span> <span class="pre">'Qwen/Qwen2.5-7B',</span> <span class="pre">'Qwen/Qwen2.5-7B-Instruct',</span> <span class="pre">'Qwen/Qwen2.5-14B',</span> <span class="pre">'Qwen/Qwen2.5-14B-Instruct',</span> <span class="pre">'Qwen/Qwen2.5-32B',</span> <span class="pre">'Qwen/Qwen2.5-32B-Instruct',</span> <span class="pre">'Qwen/Qwen2.5-72B',</span> <span class="pre">'Qwen/Qwen2.5-72B-Instruct',</span> <span class="pre">'Qwen/QwQ-32B-Preview',</span> <span class="pre">'Qwen/Qwen3-0.6B',</span> <span class="pre">'Qwen/Qwen3-1.7B',</span> <span class="pre">'Qwen/Qwen3-4B',</span> <span class="pre">'Qwen/Qwen3-8B',</span> <span class="pre">'Qwen/Qwen3-14B',</span> <span class="pre">'microsoft/phi-1',</span> <span class="pre">'microsoft/phi-1_5',</span> <span class="pre">'microsoft/phi-2',</span> <span class="pre">'microsoft/Phi-3-mini-4k-instruct',</span> <span class="pre">'microsoft/phi-4',</span> <span class="pre">'google/gemma-2b',</span> <span class="pre">'google/gemma-7b',</span> <span class="pre">'google/gemma-2b-it',</span> <span class="pre">'google/gemma-7b-it',</span> <span class="pre">'google/gemma-2-2b',</span> <span class="pre">'google/gemma-2-2b-it',</span> <span class="pre">'google/gemma-2-9b',</span> <span class="pre">'google/gemma-2-9b-it',</span> <span class="pre">'google/gemma-2-27b',</span> <span class="pre">'google/gemma-2-27b-it',</span> <span class="pre">'01-ai/Yi-6B',</span> <span class="pre">'01-ai/Yi-34B',</span> <span class="pre">'01-ai/Yi-6B-Chat',</span> <span class="pre">'01-ai/Yi-34B-Chat',</span> <span class="pre">'google-t5/t5-small',</span> <span class="pre">'google-t5/t5-base',</span> <span class="pre">'google-t5/t5-large',</span> <span class="pre">'ai-forever/mGPT']</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES" title="Permalink to this definition">#</a></dt>
<dd><p>Official model names for models on HuggingFace.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.get_checkpoint_labels">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">get_checkpoint_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.get_checkpoint_labels" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the checkpoint labels for a given model, and the label_type
(step or token). Raises an error for models that are not checkpointed.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.get_num_params_of_pretrained">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">get_num_params_of_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.get_num_params_of_pretrained" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the number of parameters of a pretrained model, used to filter to only run code for sufficiently small models.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.get_pretrained_model_config">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">get_pretrained_model_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hf_cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fold_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">device</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_n_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.get_pretrained_model_config" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the pretrained model config as an HookedTransformerConfig object.</p>
<p>There are two types of pretrained models: HuggingFace models (where
AutoModel and AutoConfig work), and models trained by me (NeelNanda) which
aren’t as integrated with HuggingFace infrastructure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_name</strong> – The name of the model. This can be either the official
HuggingFace model name, or the name of a model trained by me
(NeelNanda).</p></li>
<li><p><strong>hf_cfg</strong> (<em>dict</em><em>, </em><em>optional</em>) – Config of a loaded pretrained HF model,
converted to a dictionary.</p></li>
<li><p><strong>checkpoint_index</strong> (<em>int</em><em>, </em><em>optional</em>) – If loading from a
checkpoint, the index of the checkpoint to load. Defaults to None.</p></li>
<li><p><strong>checkpoint_value</strong> (<em>int</em><em>, </em><em>optional</em>) – If loading from a checkpoint, the</p></li>
<li><p><strong>of</strong> (<em>value</em>) – the checkpoint to load, ie the step or token number (each model has
checkpoints labelled with exactly one of these). Defaults to None.</p></li>
<li><p><strong>fold_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to fold the layer norm into the
subsequent linear layers (see HookedTransformer.fold_layer_norm for
details). Defaults to False.</p></li>
<li><p><strong>device</strong> (<em>str</em><em>, </em><em>optional</em>) – The device to load the model onto. By
default will load to CUDA if available, else CPU.</p></li>
<li><p><strong>n_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of devices to split the model across. Defaults to 1.</p></li>
<li><p><strong>default_prepend_bos</strong> (<em>bool</em><em>, </em><em>optional</em>) – <p>Default behavior of whether to prepend the BOS token when the
methods of HookedTransformer process input text to tokenize (only when input is a string).
Resolution order for default_prepend_bos:
1. If user passes value explicitly, use that value
2. Model-specific default from cfg_dict if it exists (e.g. for bloom models it’s False)
3. Global default (True)</p>
<p>Even for models not explicitly trained with the BOS token, heads often use the
first position as a resting position and accordingly lose information from the first token,
so this empirically seems to give better results. Note that you can also locally override the default behavior
by passing in prepend_bos=True/False when you call a method that processes the input string.</p>
</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em><em>, </em><em>optional</em>) – The dtype to load the TransformerLens model in.</p></li>
<li><p><strong>kwargs</strong> – Other optional arguments passed to HuggingFace’s from_pretrained.
Also given to other HuggingFace functions when compatible.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="transformer_lens.past_key_value_caching.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">transformer_lens.past_key_value_caching</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="transformer_lens.hook_points.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">transformer_lens.hook_points</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Neel Nanda
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">transformer_lens.loading_from_pretrained</a><ul>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config"><code class="docutils literal notranslate"><span class="pre">Config</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_head"><code class="docutils literal notranslate"><span class="pre">Config.d_head</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_mlp"><code class="docutils literal notranslate"><span class="pre">Config.d_mlp</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_model"><code class="docutils literal notranslate"><span class="pre">Config.d_model</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_vocab"><code class="docutils literal notranslate"><span class="pre">Config.d_vocab</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.debug"><code class="docutils literal notranslate"><span class="pre">Config.debug</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.init_range"><code class="docutils literal notranslate"><span class="pre">Config.init_range</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.layer_norm_eps"><code class="docutils literal notranslate"><span class="pre">Config.layer_norm_eps</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.n_ctx"><code class="docutils literal notranslate"><span class="pre">Config.n_ctx</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.n_heads"><code class="docutils literal notranslate"><span class="pre">Config.n_heads</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.n_layers"><code class="docutils literal notranslate"><span class="pre">Config.n_layers</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.MODEL_ALIASES"><code class="docutils literal notranslate"><span class="pre">MODEL_ALIASES</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.NON_HF_HOSTED_MODEL_NAMES"><code class="docutils literal notranslate"><span class="pre">NON_HF_HOSTED_MODEL_NAMES</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES"><code class="docutils literal notranslate"><span class="pre">OFFICIAL_MODEL_NAMES</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.get_checkpoint_labels"><code class="docutils literal notranslate"><span class="pre">get_checkpoint_labels()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.get_num_params_of_pretrained"><code class="docutils literal notranslate"><span class="pre">get_num_params_of_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.get_pretrained_model_config"><code class="docutils literal notranslate"><span class="pre">get_pretrained_model_config()</span></code></a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    </body>
</html>